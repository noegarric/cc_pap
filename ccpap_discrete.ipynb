{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWfmtjRu9JEm"
      },
      "source": [
        "# Solving the CC-PAP in a finite A and finite $\\mathcal{X}$ instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQYn3LX49JEp"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoI9XHAk9JEu"
      },
      "source": [
        "Let us work on the instance on which we successfully solved the discrete PAP\n",
        "\n",
        "- $A = \\{1,2,4,6\\}$\n",
        "- $\\mathcal{X} = \\{1,2,3,4,5\\}$\n",
        "- $\\forall x \\in \\mathcal{X},\\, \\forall a \\in A, \\quad F(x|a) = \\mathbb{P}^a[X \\leq x] = (\\frac{x}{5})^a$\n",
        "-\n",
        "$$\n",
        "\\begin{cases}\n",
        "U_A(s,a) = u(s) - a^2\\\\\n",
        "U_P(x,s) = x - s\n",
        "\\end{cases}\n",
        "$$\n",
        "-\n",
        "$$\n",
        "u \\colon \\left | \\begin{matrix}\n",
        "\\mathbb{R}_+^* \\to \\mathbb{R}_-^*\\\\\n",
        "w \\mapsto -\\frac{1}{\\sqrt{w}}\n",
        "\\end{matrix} \\right.\n",
        "$$\n",
        "-\n",
        "$$\n",
        "u^{-1} = \\phi \\colon \\left | \\begin{matrix}\n",
        "\\mathbb{R}_-^* \\to \\mathbb{R}_+^*\\\\\n",
        "v \\mapsto \\frac{1}{v^2}\n",
        "\\end{matrix} \\right.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxb7T2ME9JEw"
      },
      "source": [
        "Let us copy some useful objects and functions from the previous notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KDOiNslH9JEy"
      },
      "outputs": [],
      "source": [
        "# bounds and parameters of the problem\n",
        "\n",
        "set_a = [1,2,4,6]\n",
        "set_x = [1,2,3,4,5]\n",
        "extended_set_x = np.arange(max(set_x)+1)\n",
        "max_set_x = max(set_x)\n",
        "\n",
        "bounds = sp.optimize.Bounds(lb=len(set_x)*[-np.inf], ub = len(set_x)*[-1e-9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xhnb2MxT9JEz"
      },
      "outputs": [],
      "source": [
        "def prob_vect(a):\n",
        "    cdf_vect = (extended_set_x/max_set_x)**a\n",
        "    return cdf_vect[1:]-cdf_vect[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOiEmSCm9JE0"
      },
      "source": [
        "We will reuse `get_agent_rationality_const` and `get_expected_up_function`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8M5sDrTK9JE1"
      },
      "outputs": [],
      "source": [
        "def get_agent_rationality_const(i:int):\n",
        "\n",
        "    other_a_list = []\n",
        "\n",
        "    for j in range(len(set_a)):\n",
        "        if j != i:\n",
        "            other_a_list.append(j)\n",
        "\n",
        "    matrix = np.stack([prob_vect(set_a[i]) - prob_vect(set_a[i_a]) for i_a in other_a_list])\n",
        "\n",
        "    return sp.optimize.LinearConstraint(\n",
        "                A = matrix,\n",
        "                lb = [set_a[i]**2 - set_a[i_a]**2 for i_a in other_a_list]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0k4Dezig9JE4"
      },
      "outputs": [],
      "source": [
        "def get_expected_up_function(a):\n",
        "    return lambda v:np.dot(prob_vect(a),np.pow(v,-2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iar-Rjqn9JE5"
      },
      "source": [
        "We have defined everything to solve our problem. Let us use a solver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPLGV68_9JE6"
      },
      "source": [
        "## Solving the PAP (once again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TEOowgc9JE7"
      },
      "source": [
        "First, let us write a function that solves all the subproblems of the PAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pIb0FCMs9JE8"
      },
      "outputs": [],
      "source": [
        "def solve_full_pap(R0):\n",
        "\n",
        "    # solving |A| different instances of the problem\n",
        "\n",
        "    current_max = -np.inf\n",
        "    current_res = None\n",
        "    current_i_a = -1\n",
        "\n",
        "    for i in range(len(set_a)):\n",
        "\n",
        "        const_list = [sp.optimize.LinearConstraint(\n",
        "        A = prob_vect(set_a[i]),\n",
        "        lb = [R0 + set_a[i]**2]\n",
        "        )]\n",
        "\n",
        "        const_list.append(get_agent_rationality_const(i))\n",
        "\n",
        "        # we will solve each subproblem twice (to improve numerical stability)\n",
        "\n",
        "        for _ in range(5):\n",
        "\n",
        "            init = np.random.randn(len(set_x)) + 5*np.ones(len(set_x))\n",
        "\n",
        "            res = sp.optimize.minimize(fun = get_expected_up_function(set_a[i]), bounds=bounds,\n",
        "                                        x0 = init,method=\"trust-constr\",constraints=const_list,\n",
        "                                        options={\"factorization_method\":\"SVDFactorization\"})\n",
        "\n",
        "            benef_principal = np.dot(np.array(set_x),prob_vect(set_a[i]))\n",
        "\n",
        "            if (benef_principal - res.fun > current_max) and (res.success == True):\n",
        "                current_max = benef_principal - res.fun\n",
        "                current_res = res\n",
        "                current_i_a = i\n",
        "\n",
        "    return current_res,current_i_a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFnqJ8Ko9JE9"
      },
      "source": [
        "To assess its well-working, let us compare its result for $R_0 = -30$ with the solution found at the end of `pap_cfdc.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6grGlUtW9JE9",
        "outputId": "0c756f79-e108-48db-c110-84523f0e79c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           message: `gtol` termination condition is satisfied.\n",
            "           success: True\n",
            "            status: 1\n",
            "               fun: 0.005476342569531141\n",
            "                 x: [-3.230e+02 -1.436e+01 -1.369e+01 -1.351e+01 -1.344e+01]\n",
            "               nit: 152\n",
            "              nfev: 942\n",
            "              njev: 157\n",
            "              nhev: 0\n",
            "          cg_niter: 262\n",
            "      cg_stop_cond: 4\n",
            "              grad: [ 9.515e-11  1.622e-05  8.113e-05  2.272e-04  4.868e-04]\n",
            "   lagrangian_grad: [ 1.015e-09 -1.091e-09 -2.334e-09  3.609e-09 -1.259e-09]\n",
            "            constr: [array([-1.400e+01]), array([ 6.160e+01,  1.200e+01, -5.140e-01]), array([-3.230e+02, -1.436e+01, -1.369e+01, -1.351e+01,\n",
            "                           -1.344e+01])]\n",
            "               jac: [array([[ 1.600e-03,  2.400e-02,  1.040e-01,\n",
            "                             2.800e-01,  5.904e-01]]), array([[-1.984e-01, -1.760e-01, ...,  8.000e-02,\n",
            "                             3.904e-01],\n",
            "                           [-3.840e-02, -9.600e-02, ..., -5.551e-17,\n",
            "                             2.304e-01],\n",
            "                           [ 1.536e-03,  1.997e-02, ...,  6.451e-02,\n",
            "                            -1.475e-01]]), array([[ 1.000e+00,  0.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  1.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           ...,\n",
            "                           [ 0.000e+00,  0.000e+00, ...,  1.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  0.000e+00, ...,  0.000e+00,\n",
            "                             1.000e+00]])]\n",
            "       constr_nfev: [0, 0, 0]\n",
            "       constr_njev: [0, 0, 0]\n",
            "       constr_nhev: [0, 0, 0]\n",
            "                 v: [array([-8.113e-04]), array([-2.197e-10, -3.383e-05, -5.262e-10]), array([ 3.169e-11,  7.186e-10,  7.606e-10,  7.383e-10,\n",
            "                            7.691e-10])]\n",
            "            method: tr_interior_point\n",
            "        optimality: 3.609240778022447e-09\n",
            "  constr_violation: 0.0\n",
            "    execution_time: 0.23253154754638672\n",
            "         tr_radius: 37451.104427159386\n",
            "    constr_penalty: 1.0\n",
            " barrier_parameter: 1.0240000000000006e-08\n",
            " barrier_tolerance: 1.0240000000000006e-08\n",
            "             niter: 152\n"
          ]
        }
      ],
      "source": [
        "res,i_a = solve_full_pap(-30)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGXVBkq79JE_"
      },
      "source": [
        "Indeed, it is the same solution than in the previous notebook.\n",
        "\n",
        "$\\forall w \\in \\mathbb{R}_+^*, \\, \\forall a \\in A, \\, U_A(w,a) < 0$, thus by decrease of $R_0 \\mapsto F_1(R_0)$, it exists $\\tilde{R_0}$ such that\n",
        "$$\n",
        "\\{ R_0 \\in \\mathbb{R} \\, | \\, F_1 (R_0) \\cap F_2 \\neq \\emptyset \\} = \\begin {cases}\n",
        "]-\\infty,\\tilde{R_0}[\\\\\n",
        "\\text{or}\\\\\n",
        "]-\\infty,\\tilde{R_0}]\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Let us compute an approximation of $\\tilde{R_0}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsHLdGCk9JFA",
        "outputId": "eea3786a-8b31-4c95-aba8-9abaa7634240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R0 = -10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-23.66930835  -5.65513539  -5.32572253  -5.20595773  -5.14388096]\n",
            "wages = [0.00178496 0.031269   0.0352568  0.03689765 0.03779359]\n",
            "avg Ua = -9.999998396635748\n",
            "\n",
            "R0 = -5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-3.99999938 -3.99999955 -3.99999968 -3.99999978 -3.99999996]\n",
            "wages = [0.06250002 0.06250001 0.06250001 0.06250001 0.0625    ]\n",
            "avg Ua = -4.999999672318925\n",
            "\n",
            "R0 = -3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-1.99999995 -1.99999995 -1.99999995 -1.99999996 -1.99999999]\n",
            "wages = [0.25000001 0.25000001 0.25000001 0.25000001 0.25      ]\n",
            "avg Ua = -2.999999959039942\n",
            "\n",
            "R0 = -2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-1.         -1.         -1.         -1.         -0.99999999]\n",
            "wages = [1.00000001 1.00000001 1.00000001 1.00000001 1.00000002]\n",
            "avg Ua = -1.999999994879999\n",
            "\n",
            "R0 = -1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfeasible problem\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for R0 in [-10,-5,-3,-2,-1]:\n",
        "    print(f\"R0 = {R0}\")\n",
        "    res,i_a = solve_full_pap(R0)\n",
        "    if res != None:\n",
        "        print(res.success)\n",
        "        print(f\"utilities = {res.x}\")\n",
        "        print(f\"wages = {np.pow(res.x,-2)}\")\n",
        "        print(f\"avg Ua = {np.dot(prob_vect(set_a[i_a]),res.x) - set_a[i_a]**2}\")\n",
        "    else:\n",
        "        print(\"Unfeasible problem\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCNr4Sc89JFB"
      },
      "source": [
        "We have shown that $-2 \\leq \\tilde{R_0} \\leq -1$. We can therefore safely plot the solution of the PAP for $R_0 \\in [-30,-2]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q27aQBlM9JFC",
        "outputId": "fb294f72-6b63-43d6-d708-815e89ddd41a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        }
      ],
      "source": [
        "R0_list = np.linspace(-30,-2,20)\n",
        "res_list_1 = []\n",
        "i_max_list_1 = []\n",
        "up_avg_list_1 = []\n",
        "\n",
        "for i in range(len(R0_list)):\n",
        "    res,i_max = solve_full_pap(R0_list[i])\n",
        "    res_list_1.append(res)\n",
        "    i_max_list_1.append(i_max)\n",
        "    up_avg_list_1.append(np.dot(set_x,prob_vect(set_a[i_max])) - res.fun)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PqxuWrUj9JFC",
        "outputId": "3ab3cc2d-0459-4248-cb2a-cda44bf14bd3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.title(\"Solutions of the discrete PAP\")\n",
        "\n",
        "plt.plot(R0_list,up_avg_list_1,marker=\"+\")\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel(\"R0\")\n",
        "plt.ylabel(\"Average principal utility\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ncG7fmN9JFE"
      },
      "source": [
        "The principal utility is a decreasing function of $R_0$, which is guaranteed by the theory. It makes us confident in the well-working of our code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga1V5-F39JFE"
      },
      "source": [
        "## Solving the ethical CC-PAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDe7a10X9JFE"
      },
      "source": [
        "Now that we are confident with the well-working of the PAP, let us solve the ethical CC-PAP. For computational reasons, we start by this problem (it has 5 subproblems instead of 31)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TQldzrhs9JFF"
      },
      "outputs": [],
      "source": [
        "# constraint enforcing increasing wages\n",
        "\n",
        "monot_const_mat = np.zeros((len(set_x)-1,len(set_x)))\n",
        "\n",
        "for i in range(len(set_x)-1):\n",
        "\n",
        "    monot_const_mat[i,i] = -1\n",
        "    monot_const_mat[i,i+1] = 1\n",
        "\n",
        "monot_const = sp.optimize.LinearConstraint(\n",
        "    A = monot_const_mat,\n",
        "    lb = np.zeros(len(set_x)-1),\n",
        "    ub = (len(set_x)-1) * [np.inf]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifmIYMJD9JFF"
      },
      "outputs": [],
      "source": [
        "def solve_ethical_cc_pap(R1,alpha,other_init=False,loc_init=-np.ones(len(set_x)),num_sol=1):\n",
        "\n",
        "    # solving |A| * |X| * num_sol different instances of the problem\n",
        "\n",
        "    # experimentally, the optimal point is close to this initial value \n",
        "    # for big R1 values, which are the toughest to handle\n",
        "\n",
        "    if other_init:\n",
        "        init = loc_init\n",
        "    else:\n",
        "        init = np.ones(len(set_x))*(R1+1)\n",
        "\n",
        "    current_max = -np.inf\n",
        "    current_res = None\n",
        "    current_i_a = -1\n",
        "\n",
        "    for i in range(len(set_a)):\n",
        "\n",
        "        const_list = [get_agent_rationality_const(i),monot_const]\n",
        "\n",
        "        for j in range(len(set_x)):\n",
        "\n",
        "            feasible = False\n",
        "\n",
        "            if j == 0:\n",
        "\n",
        "                feasible = True\n",
        "\n",
        "                matrix = np.zeros(len(set_x))\n",
        "                matrix[0] = 1\n",
        "\n",
        "                const_list.append(sp.optimize.LinearConstraint(\n",
        "                    A = matrix,\n",
        "                    lb = R1 + set_a[i]**2\n",
        "                ))\n",
        "\n",
        "            elif np.pow(set_x[j-1]/max_set_x,set_a[i]) <= alpha:\n",
        "\n",
        "                feasible = True\n",
        "\n",
        "                matrix = np.zeros((2,len(set_x)))\n",
        "                matrix[0,j-1] = 1\n",
        "                matrix[1,j] = 1\n",
        "\n",
        "                const_list.append(sp.optimize.LinearConstraint(\n",
        "                    A = matrix,\n",
        "                    lb = [-np.inf,R1 + set_a[i]**2],\n",
        "                    ub = [R1 + set_a[i]**2,np.inf]\n",
        "                ))\n",
        "\n",
        "\n",
        "            if feasible:\n",
        "\n",
        "                for _ in range(num_sol):\n",
        "\n",
        "                    res = sp.optimize.minimize(fun = get_expected_up_function(set_a[i]), bounds=bounds,\n",
        "                                                x0 = init,method=\"trust-constr\",constraints=const_list,\n",
        "                                                options={\"factorization_method\":\"SVDFactorization\"})\n",
        "\n",
        "                    benef_principal = np.dot(np.array(set_x),prob_vect(set_a[i]))\n",
        "\n",
        "                    if (benef_principal - res.fun > current_max) and (res.success == True):\n",
        "                        current_max = benef_principal - res.fun\n",
        "                        current_res = res\n",
        "                        current_i_a = i\n",
        "\n",
        "    return current_res,current_i_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiBpY6kD9JFG",
        "outputId": "d5eeb6e9-32f4-4c8d-e9fc-2ae8ded8c9d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a = 1\n",
            "x = [-8.99950035 -8.99917123 -8.99840366 -8.99669792 -8.99476208]\n",
            "Average Up =  0.012\n",
            "\n",
            "           message: `gtol` termination condition is satisfied.\n",
            "           success: True\n",
            "            status: 1\n",
            "               fun: 0.012351973512830407\n",
            "                 x: [-9.000e+00 -8.999e+00 -8.998e+00 -8.997e+00 -8.995e+00]\n",
            "               nit: 21\n",
            "              nfev: 84\n",
            "              njev: 14\n",
            "              nhev: 0\n",
            "          cg_niter: 24\n",
            "      cg_stop_cond: 4\n",
            "              grad: [ 5.488e-04  5.488e-04  5.490e-04  5.493e-04  5.497e-04]\n",
            "   lagrangian_grad: [ 4.606e-11 -1.372e-10 -1.484e-10  8.670e-10  4.499e-10]\n",
            "            constr: [array([-9.560e-04, -1.911e-03, -2.355e-03]), array([ 3.291e-04,  7.676e-04,  1.706e-03,  1.936e-03]), array([-9.000e+00]), array([-9.000e+00, -8.999e+00, -8.998e+00, -8.997e+00,\n",
            "                           -8.995e+00])]\n",
            "               jac: [array([[ 1.600e-01,  8.000e-02, ..., -8.000e-02,\n",
            "                            -1.600e-01],\n",
            "                           [ 1.984e-01,  1.760e-01, ..., -8.000e-02,\n",
            "                            -3.904e-01],\n",
            "                           [ 1.999e-01,  1.960e-01, ..., -1.549e-02,\n",
            "                            -5.379e-01]]), array([[-1.000e+00,  1.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00, -1.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  0.000e+00, ...,  1.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  0.000e+00, ..., -1.000e+00,\n",
            "                             1.000e+00]]), array([[ 1.000e+00,  0.000e+00,  0.000e+00,\n",
            "                             0.000e+00,  0.000e+00]]), array([[ 1.000e+00,  0.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  1.000e+00, ...,  0.000e+00,\n",
            "                             0.000e+00],\n",
            "                           ...,\n",
            "                           [ 0.000e+00,  0.000e+00, ...,  1.000e+00,\n",
            "                             0.000e+00],\n",
            "                           [ 0.000e+00,  0.000e+00, ...,  0.000e+00,\n",
            "                             1.000e+00]])]\n",
            "       constr_nfev: [0, 0, 0, 0]\n",
            "       constr_njev: [0, 0, 0, 0]\n",
            "       constr_nhev: [0, 0, 0, 0]\n",
            "                 v: [array([-4.268e-07, -8.534e-08, -3.657e-08]), array([-2.197e-03, -1.649e-03, -1.099e-03, -5.499e-04]), array([-2.746e-03]), array([ 1.422e-07,  1.422e-07,  1.422e-07,  1.423e-07,\n",
            "                            1.423e-07])]\n",
            "            method: tr_interior_point\n",
            "        optimality: 8.669775503150027e-10\n",
            "  constr_violation: 0.0\n",
            "    execution_time: 0.03793501853942871\n",
            "         tr_radius: 3088742.0305302786\n",
            "    constr_penalty: 1.0\n",
            " barrier_parameter: 1.2800000000000007e-06\n",
            " barrier_tolerance: 1.2800000000000007e-06\n",
            "             niter: 21\n"
          ]
        }
      ],
      "source": [
        "res,imax = solve_ethical_cc_pap(R1=-10,alpha=0.05)\n",
        "a = set_a[imax]\n",
        "prob = prob_vect(a)\n",
        "\n",
        "print(f\"a = {a}\")\n",
        "print(f\"x = {res.x}\")\n",
        "print(f\"Average Up = {res.fun : 1.3f}\")\n",
        "print()\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctmuxkbv9JFG"
      },
      "source": [
        "To check this solution, we will compute the average agent utility for $\\texttt{res.x}$ and all values of $a$, and $\\mathbb{P}^a [U_A(w(X),a) \\geq R_0]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXlilc7T9JFH",
        "outputId": "5b287930-3dcc-4b6c-f470-f5063e4b7000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a = 1\n",
            "Utility = -9.997707049627499\n",
            "\n",
            "a = 2\n",
            "Utility = -12.996751062414376\n",
            "\n",
            "a = 4\n",
            "Utility = -24.99579624410662\n",
            "\n",
            "a = 6\n",
            "Utility = -44.995352301282324\n",
            "\n",
            "probability Ua >= R0 = 1.0\n"
          ]
        }
      ],
      "source": [
        "def get_expected_ua_orig_variables(a):\n",
        "    return lambda w:np.dot(prob_vect(a),-np.pow(w,-0.5))-a**2\n",
        "\n",
        "for i in range(len(set_a)):\n",
        "    print(f\"a = {set_a[i]}\")\n",
        "    print(f\"Utility = {get_expected_ua_orig_variables(set_a[i])(np.pow(res.x,-2))}\")\n",
        "    print()\n",
        "\n",
        "opt_prob_vect = prob_vect(set_a[imax])\n",
        "prob_higher = np.sum(\n",
        "    opt_prob_vect * (res.x >= -10 - 1e-4)\n",
        ")\n",
        "print(f\"probability Ua >= R0 = {prob_higher}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1tI0gZs9JFH"
      },
      "source": [
        "The solution we have found satisfies both constraints, which is encouraging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JvP81cn9JFI"
      },
      "source": [
        "Like the PAP, let us define $\\tilde{R_1}(\\alpha)$ by\n",
        "$$\n",
        "\\{ R_0 \\in \\mathbb{R} \\, | \\text{the ethical CC-PAP of parameters } (R_0,\\alpha) \\text{ is feasible} \\} = \\begin {cases}\n",
        "]-\\infty,\\tilde{R_1}(\\alpha)[\\\\\n",
        "\\text{or}\\\\\n",
        "]-\\infty,\\tilde{R_1}(\\alpha)]\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Let us compute an approximation of $\\tilde{R_1}(0.95)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEXOiXVz9JFI",
        "outputId": "04679fcf-7aec-490b-c81b-12a2b0cd5e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R1 = -10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-8.99950035 -8.99917123 -8.99840366 -8.99669792 -8.99476208]\n",
            "wages = [0.01234705 0.01234795 0.01235006 0.01235474 0.01236006]\n",
            "avg Ua = -9.997707049627499\n",
            "\n",
            "R1 = -5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-3.99978    -3.99924846 -3.99913674 -3.99876049 -3.99720577]\n",
            "wages = [0.06250688 0.06252349 0.06252699 0.06253875 0.06258741]\n",
            "avg Ua = -4.998826292052333\n",
            "\n",
            "R1 = -3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-1.99986092 -1.99969593 -1.99948478 -1.99917251 -1.99842012]\n",
            "wages = [0.25003477 0.25007603 0.25012885 0.250207   0.25039544]\n",
            "avg Ua = -2.999326850592241\n",
            "\n",
            "R1 = -2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "utilities = [-0.9999878  -0.99997351 -0.9999389  -0.99989916 -0.99980283]\n",
            "wages = [1.00002441 1.00005298 1.00012221 1.0002017  1.00039446]\n",
            "avg Ua = -1.9999204396502919\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for R1 in [-10,-5,-3,-2]:\n",
        "    print(f\"R1 = {R1}\")\n",
        "    res,i_a = solve_ethical_cc_pap(R1,alpha=0.05)\n",
        "    if res != None:\n",
        "        print(res.success)\n",
        "        print(f\"utilities = {res.x}\")\n",
        "        print(f\"wages = {np.pow(res.x,-2)}\")\n",
        "        print(f\"avg Ua = {np.dot(prob_vect(set_a[i_a]),res.x) - set_a[i_a]**2}\")\n",
        "    else:\n",
        "        print(\"Unfeasible problem\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAvGLJQl9JFJ"
      },
      "source": [
        "This code cell shows us that $\\tilde{R_1}(0.95) \\geq -2$.\n",
        "\n",
        "More rigorously, we can point out that for $R_1 < -1$, we can take $v = (R_1+1, \\dots, R_1+1)$ and $a = 1$. For any $b \\in A$ the average agent utility in $v,b$ is $R_1 + 1 - b^2$ : this function of $b$ is maximal in $1$. Moreover, the average agent utility in $v,a$ is equal to $R_1$ a.s., which verifies the probability constraint for any $\\alpha$.\n",
        "\n",
        "Thus, $\\forall \\alpha \\in [0,1], \\, \\tilde{R_1}(\\alpha) \\geq -1$. We can thus safely solve the CC-PAP of any $R_1 \\leq -2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebYnlb2D9JFJ",
        "outputId": "e4b928dc-197d-47e7-c670-19dbe4c5711d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 12,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 12,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 12,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 11,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 11,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 11,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 10,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 10,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 10,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 9,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 9,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 9,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 8,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 8,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 8,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 7,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 7,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 7,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 6,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 6,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 6,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 5,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 5,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 5,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 4,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 4,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 4,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 3,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 3,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 3,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 2,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 2,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 2,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 1,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 1,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 1,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 0,0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 0,1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "i,j = 0,2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py:317: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
          ]
        }
      ],
      "source": [
        "R1_list = np.concatenate((np.linspace(-30,-5,10),np.array([-4,-3,-2])))\n",
        "alpha_list = [0.05,0.1,0.5]\n",
        "\n",
        "res_array = []\n",
        "i_max_array = []\n",
        "up_avg_array = []\n",
        "\n",
        "\n",
        "for i in range(len(R1_list)-1,-1,-1):\n",
        "\n",
        "    res_list_fixed_R1 = []\n",
        "    i_max_list_fixed_R1 = []\n",
        "    up_avg_list_fixed_R1 = []\n",
        "\n",
        "    for j in range(len(alpha_list)):\n",
        "\n",
        "        print()\n",
        "        print(f\"i,j = {i},{j}\")\n",
        "        print()\n",
        "\n",
        "        res,i_max = solve_ethical_cc_pap(R1=R1_list[i],alpha=alpha_list[j])\n",
        "\n",
        "\n",
        "\n",
        "        res_list_fixed_R1.append(res)\n",
        "        i_max_list_fixed_R1.append(i_max)\n",
        "        if res != None:\n",
        "            up_avg_list_fixed_R1.append(np.dot(set_x,prob_vect(set_a[i_max])) - res.fun)\n",
        "        else:\n",
        "            print(\"unsuccessful optimization\")\n",
        "            up_avg_list_fixed_R1.append(-2)\n",
        "\n",
        "    res_array.append(res_list_fixed_R1)\n",
        "    i_max_array.append(i_max_list_fixed_R1)\n",
        "    up_avg_array.append(up_avg_list_fixed_R1)\n",
        "\n",
        "res_array.reverse()\n",
        "i_max_array.reverse()\n",
        "up_avg_array.reverse()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iZgCwMgL9JFJ",
        "outputId": "a0608b80-eec7-43e9-99ad-0cccb2f8a4ad"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.title(\"Soving the discrete ethical CC-PAP\")\n",
        "\n",
        "up_avg_ndarray = np.array(up_avg_array)\n",
        "\n",
        "label_list = [f\"ethical CC-PAP, $\\\\alpha=${a}\" for a in alpha_list]\n",
        "\n",
        "plt.plot(R1_list,up_avg_array,marker=\"+\",label=label_list)\n",
        "plt.plot(R0_list,up_avg_list_1,marker=\"+\",label=\"PAP\")\n",
        "\n",
        "plt.xlabel(\"R1\")\n",
        "plt.ylabel(\"Average principal utility\")\n",
        "plt.legend()\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stagecmm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
